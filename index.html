<!DOCTYPE html>
<html>
<head>
<title>Blake Woodworth</title>
<meta charset="UTF-8">
<style>
body {
background-color:#e8eef9;
font-family: "Geneva", serif;
color: #484848
}
.centerpic {
margin-left: auto;
margin-right: auto;
width: 300px
} 
h1 {
text-align: center
}
h2,h3,p,a.al,h4,li {
margin-left: 100px;
margin-right: 100px;
}
.centertext {
text-align: center
} 

p.small {
line-height: 50%;
font-size: 70%;
text-align: right;
margin-left: 100px;
margin-right: 110px;
}

</style>
</head>
<h1>Blake Woodworth</h1>
<div class="centerpic">
<img src="blake.jpg" alt="Blake" height="169" width="300">
</div>
<p class="centertext">blake (dot) woodworth (at) gwu (dot) edu</p>
<hr width = 84%/>

<h3>About</h3>
<p>
I am an Assistant Professor in Computer Science at George Washington University since August 2023. <!--I will be accepting PhD students interested in optimization and machine learning. 
All admissions decisions will happen through the standard GWU application process, so if you are interested, please apply <a href="https://graduate.seas.gwu.edu/doctoral-admissions-requirements">here</a>.
Once you have applied, please feel free to send me an email so that I know to look out for your application.
</p>
<p>-->
The primary focus of my research is on the theory of optimization, with a particular emphasis on precisely understanding convex, non-convex, and distributed optimization algorithms. I have also been very interested in efforts to understand modern, highly overparametrized machine learning models through the lens of implicit regularization. During my PhD, I also had the opportunity to work on fairness in ML and on adaptive data analysis.
</p>
<p>
From October 2021-June 2023, I was a postdoctoral researcher with the <a href="https://www.di.ens.fr/sierra/">SIERRA team at Inria</a>, working with Francis Bach, and before that, I was a PhD student in computer science at the <a href="http://www.ttic.edu/">Toyota Technological Institute at Chicago</a> (TTIC) advised by <a href="http://ttic.uchicago.edu/~nati/">Nati Srebro</a> until I graduated in Summer 2021. Before TTIC, I studied at Yale University where I received a B.S. in computer science, advised by Dan Spielman. At Yale, my coursework was spread evenly across the computer science, mathematics, and statistics departments; I was also a peer tutor for several programming-intensive computer science courses.
</p>
<p>
From September 2017-July 2019 I was supported by a NSF Graduate Research Fellowship, and from July 2019-August 2021, I was supported by a Google PhD Fellowship in machine learning.
</p>

<p><a href="Woodworth_CV.pdf">My CV</a>.</p>
<p> <a href="https://scholar.google.com/citations?hl=en&user=BnQdO2UAAAAJ&view_op=list_works&sortby=pubdate">My Google Scholar page</a>.</p>

<hr width = 84%/>
<h3>Publications</h3>

<ul>
  <li><a href="https://arxiv.org/abs/2501.13790"><b>Local Steps Speed Up Local GD for Heterogeneous Distributed Logistic Regression</b></a>
    <br>
    Michael Crawshaw, Blake Woodworth, Mingrui Liu
    <br>
    <i>ICLR, 2025</i>
  </li>
</ul>

<ul>
  <li><a href="https://arxiv.org/abs/2302.03542"><b>Two Losses Are Better Than One: Faster Optimization Using a Cheaper Proxy</b></a>
    <br>
    Blake Woodworth, Konstantin Mishchenko, Francis Bach
    <br>
    <i>ICML, 2023</i>
  </li>
</ul>

<ul>
  <li><a href="https://arxiv.org/abs/2206.07638"><b>Asynchronous SGD Beats Minibatch SGD Under Arbitrary Delays</b></a>
    <br>
    Konstantin Mishchenko, Francis Bach, Mathieu Even, Blake Woodworth
    <br>
    <i>NeurIPS, 2022</i>
  </li>
</ul>

<ul>
  <li><a href="https://arxiv.org/abs/2204.04970"><b>Non-Convex Optimization with Certificates and Fast Rates Through Kernel Sums of Squares</b></a>
    <br>
    Blake Woodworth, Francis Bach, Alessandro Rudi
    <br>
    <i>COLT, 2022</i>
  </li>
</ul>

<ul>
  <li><a href="https://link.springer.com/article/10.1007/s10107-022-01822-7"><b>Lower Bounds for Non-Convex Stochastic Optimization</b></a>
    <br>
    Yossi Arjevani, Yair Carmon, John C. Duchi, Dylan J. Foster, Nathan Srebro, Blake Woodworth
    <br>
    <i>Mathematical Programming, 2022</i>
    <br>
    <a href="https://arxiv.org/abs/1912.02365"><i>arXiv version</i></a>
  </li>
</ul>

<ul>
  <li><a href="https://arxiv.org/abs/2109.00534"><b>The Minimax Complexity of Distributed Optimization</b></a>
    <br>
    Blake Woodworth
    <br>
    <i>PhD Thesis, 2021</i>
  </li>
</ul>

<ul>
  <li><a href="https://arxiv.org/abs/2110.02954"><b>A Stochastic Newton Algorithm for Distributed Convex Optimization</b></a>
    <br>
    Brian Bullins, Kumar Kshitij Patel, Ohad Shamir, Nathan Srebro, Blake Woodworth
    <br>
    <i>NeurIPS, 2021</i>
  </li>
</ul>

<ul>
  <li><a href="https://arxiv.org/abs/2106.02720"><b>An Even More Optimal Stochastic Optimization Algorithm: Minibatching and Interpolation Learning</b></a>
    <br>
    Blake Woodworth, Nathan Srebro
    <br>
    <i>NeurIPS, 2021</i>
  </li>
</ul>

<ul>
  <li><a href="https://arxiv.org/abs/2102.09769"><b>On the Implicit Bias of Initialization Shape: Beyond Infinitesimal Mirror Descent</b></a>
    <br>
    Shahar Azulay, Edward Moroshko, Mor Shpigel Nacson, Blake Woodworth, Nathan Srebro, Amir Globerson, Daniel Soudry
    <br>
    <i>ICML, 2021</i>
  </li>
</ul>

<ul>
  <li><a href="https://arxiv.org/abs/2102.01583"><b>The Min-Max Complexity of Distributed Stochastic Convex Optimization with Intermittent Communication</b></a>
    <br>
    Blake Woodworth, Brian Bullins, Ohad Shamir, Nathan Srebro
    <br>
    Best Paper Award
    <br>
    <i>COLT, 2021</i>
  </li>
</ul>

<ul>
  <li><a href="https://arxiv.org/abs/2004.01025"><b>Mirrorless Mirror Descent: A More Natural Discretization of Riemannian Gradient Flow</b></a>
    <br>
    Suriya Gunasekar, Blake Woodworth, Nathan Srebro
    <br>
    <i>AISTATS, 2021</i>
  </li>
</ul>

<ul>
  <li><a href="https://arxiv.org/abs/2007.06738"><b>Implicit Bias in Deep Linear Classification: Initialization Scale vs Training Accuracy</b></a>
    <br>
    Edward Moroshko, Suriya Gunasekar, Blake Woodworth, Jason D. Lee, Nathan Srebro, Daniel Soudry
    <br>
    <i>NeurIPS, 2020</i>
  </li>
</ul>

<ul>
  <li><a href="https://arxiv.org/abs/2006.04735"><b>Minibatch vs Local SGD for Heterogeneous Distributed Learning</b></a>
    <br>
    Blake Woodworth, Kumar Kshitij Patel, Nathan Srebro
    <br>
    <i>NeurIPS, 2020</i>
  </li>
</ul>

<ul>
  <li><a href="https://arxiv.org/abs/2002.07839"><b>Is Local SGD Better than Minibatch SGD?</b></a>
    <br>
    Blake Woodworth, Kumar Kshitij Patel, Sebastian U. Stich, Zhen Dai, Brian Bullins, H. Brendan McMahan, Ohad Shamir, Nathan Srebro
    <br>
    <i>ICML, 2020</i>
    <br>
    <a href="./code/local_sgd.py">Python code</a>
  </li>
</ul>

<ul>
  <li><a href="https://arxiv.org/abs/2002.09277"><b>Kernel and Deep Regimes in Overparametrized Models</b></a>
    <br>
    Blake Woodworth, Suriya Gunasekar, Jason D. Lee, Edward Moroshko, Pedro Savarese, Itay Golan, Daniel Soudry, Nathan Srebro
    <br>
    <i>COLT, 2020</i>
  </li>
</ul>

<ul>
  <li><a href="https://arxiv.org/abs/1911.02212"><b>The Gradient Complexity of Linear Regression</b></a>
    <br>
    Mark Braverman, Elad Hazan, Max Simchowitz, Blake Woodworth
    <br>
    <i>COLT, 2020</i>
  </li> 
</ul>

<ul>
  <li><a href="https://arxiv.org/abs/1906.09231"><b>Guaranteed Validity for Empirical Approaches to Adaptive Data Analysis</b></a>
    <br>
    Ryan Rogers, Aaron Roth, Adam Smith, Nathan Srebro, Om Thakkar, Blake Woodworth
    <br>
    <i>AISTATS, 2020</i>
  </li>
</ul>

<ul>
  <li><a href="https://arxiv.org/abs/1907.00762"><b>Open Problem: The Oracle Complexity of Convex Optimization with Limited Memory</b></a>
    <br>
    Blake Woodworth, Nathan Srebro
    <br>
    <i>COLT, 2019</i>
  </li>
</ul>

<ul>
  <li><a href="https://arxiv.org/abs/1902.04686"><b>The Complexity of Making the Gradient Small in Stochastic Convex Optimization</b></a>
    <br>
    Dylan Foster, Ayush Sekhari, Ohad Shamir, Nathan Srebro, Karthik Sridharan, Blake Woodworth
    <br>
    Best Student Paper Award
    <br>
    <i>COLT, 2019</i>
  </li>
</ul>

<ul>
  <li><a href="https://arxiv.org/abs/1805.10222"><b>Graph Oracle Models, Lower Bounds, and Gaps for Parallel Stochastic Optimization</b></a>
    <br>
    Blake Woodworth, Jialei Wang, Adam Smith, Brendan McMahan, and Nathan Srebro
    <br>
    <i>NeurIPS, 2018</i>
  </li>
</ul>

<ul>
  <li><a href="https://arxiv.org/abs/1807.00028"><b>Training Well-Generalizing Classifiers for Fairness Metrics and Other Data-Dependent Constraints</b></a>
    <br>
    Andrew Cotter, Maya Gupta, Heinrich Jiang, Nathan Srebro, Karthik Sridharan, Serena Wang, Blake Woodworth, and Seungil You
    <br>
    <i>FAT/ML 2018, ICML 2019</i>
  </li>
</ul>


<ul>
  <li><a href="https://arxiv.org/abs/1803.04307"><b>The Everlasting Database: Statistical Validity at a Fair Price</b></a>
    <br>
    Blake Woodworth, Vitaly Feldman, Saharon Rosset, and Nathan Srebro
    <br>
    <i>NeurIPS, 2018</i>
  </li>
</ul>


<ul>
  <li><a href="http://arxiv.org/abs/1709.03594"><b>Lower Bound for Randomized First Order Convex Optimization</b></a>
    <br>
      Blake Woodworth and Nathan Srebro
    <br>
    <i>arXiv, 2017</i>
  </li>
</ul>

<ul>
  <li><a href="https://arxiv.org/abs/1705.09280"><b>Implicit Regularization in Matrix Factorization</b></a>
    <br>
      Suriya Gunasekar, Blake Woodworth, Srinadh Bhojanapalli, Behnam Neyshabur, and Nathan Srebro
    <br>
    <i>NeurIPS, 2017</i>
  </li>
</ul>

<ul>
  <li><a href="https://arxiv.org/abs/1702.06081"><b>Learning Non-Discriminatory Predictors</b></a>
    <br>
      Blake Woodworth, Suriya Gunasekar, Mesrob I. Ohannessian, and Nathan Srebro
    <br>
    <i>COLT, 2017</i>
  </li>
</ul>

<ul>
  <li><a href="https://arxiv.org/abs/1605.08003"><b>Tight Complexity Bounds for Optimizing Composite Objectives</b></a>
    <br>
      Blake Woodworth and Nathan Srebro
    <br>
    <i>NeurIPS, 2016</i>
  </li>
</ul>


<!-- <hr width = 84%/>
<h3>Presentations</h3>
<p>
Blake Woodworth and Nathan Srebro. <b>Tight Complexity Bounds for Optimizing Composite Objectives</b>.
<ul>
<li>Invited talk at Microsoft Research New York, NY. September 2016.</li>
<li>Invited talk at Microsoft Research Redmond, WA. October 2016.</li>
<li>Machine Learning Seminar Series at TTIC, Chicago, IL. October 2016.</li>
<li>Poster presentation at NIPS 2016, Barcelona, Spain. December 2016.</li>
</ul>
</p> -->

<!-- <br>
<hr width = 84%/>
<h3>Work Experience</h3>
<ul>
<li>
Research Intern at Microsoft Research Cambridge, UK (Summer 2017)
<br>
I worked with graph-structured neural networks (e.g. <a href="https://arxiv.org/abs/1511.05493">GG-NN</a>) in collaboration with Ryota Tomioka and other members of the MSR Cambridge AMPNet team.
</li>
</ul>

<ul>
<li>
Research Intern at Google Research Princeton, NJ (Summer 2019)
<br>
I worked with Elad Hazan and Naman Agarwal on online learning and optimization algorithms.
</li>
</ul> -->


<!-- <br>
<hr width = 84%/>
<h3>Teaching Experience</h3>
<ul>
<li>
Teaching Assistant for TTIC 31120 - Statistical and Computational Learning Theory (Fall 2018)
<br>
With Professor Nathan Srebro
</li>
</ul>
<ul>
<li>
Teaching Assistant for TTIC 31070 - Convex Optimization (Winter 2018)
<br>
With Professor Nathan Srebro
</li>
</ul>
<ul>
<li>
Teaching Assistant for TTIC 31150 - Mathematical Toolkit (Fall 2016)
<br>
With Professor Madhur Tulsiani
</li>
</ul>
<ul>
<li>
Peer Tutor at Yale for CPSC 201, 223, and 323 (January 2013 - May 2015)
<br>
With Professors Dana Angluin, James Aspnes, and Stanley Eisenstat
</li>
</ul> -->

<!-- <hr width = 84%/>
<h3>Graduate Coursework</h3>
<ul>
  <li><a href="http://www.ttic.edu/courses/#ml">Introduction to Statistical Machine Learning</a> (Fall 2015) </li>
  <li><a href="http://www.ttic.edu/courses/#mt">Mathematical Toolkit</a> (Fall 2015) </li>
  <li><a href="http://www.ttic.edu/courses/#co">Convex Optimization</a> (Fall 2015) </li>
  <li><a href="http://www.ttic.edu/courses/#algo">Algorithms</a> (Winter 2016) </li>
  <li><a href="http://www.ttic.edu/courses/#cv">Introduction to Computer Vision</a> (Winter 2016) </li>
  <li><a href="http://www.ttic.edu/courses/#pgm">Probabilistic Graphical Models</a> (Spring 2016) </li>
  <li><a href="https://pcmi.ias.edu/program-index/2016"><b>Park City Mathematics Institute</b></a> (Summer 2016) </li>
  <li><a href="http://www.ttic.edu/courses/#sclt">Statistical and Computational Learning Theory</a> (Fall 2016) </li>
  <li><a href="http://collegecatalog.uchicago.edu/thecollege/mathematics/#courseinventory">Basic Functional Analysis</a> (Winter 2017) </li>
  <li><a href="http://galton.uchicago.edu/~lalley/Courses/381/index.html">Measure-Theoretic Probability I</a> (Winter 2017) </li>
  <li><a href="http://www.ttic.edu/courses/#ulda">Unsupervised Learning and Data Analysis</a> (Spring 2017)</li>
</ul>
<br>
<br> -->

<p class="small">Last Updated: March 31, 2023</p>
</html>
